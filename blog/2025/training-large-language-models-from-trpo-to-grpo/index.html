<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h4>Dive into how DeepSeek, ChatGPT, and other LLMs leverage reinforcement learning to enhance their training</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nPPVFFCtxEjhZ-64vAQhgQ.png"><figcaption>Image generated by the author using DALL-E</figcaption></figure> <p>DeepSeek has recently made <strong>quite a buzz </strong>in the AI community, thanks to its impressive performance at relatively low costs. I think this is a perfect opportunity to dive deeper into how Large Language Models (LLMs) are trained. In this article, we will focus on the Reinforcement Learning (RL) side of things: we will cover TRPO, PPO, and, more recently, GRPO (don't worry, I will explain all these terms soon!)</p> <p>I have aimed to keep this article relatively easy to read and accessible, by minimizing the math, so you won’t need a deep Reinforcement Learning background to follow along. However, I will assume that you have some familiarity with Machine Learning, Deep Learning, and a basic understanding of how LLMs work.</p> <p>I hope you enjoy the article, and feel free to leave a clap!</p> <h3>The 3 steps of LLM training</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gDF9kq-aLcUvG0XFBU2DGA.png"><figcaption>The 3 steps of LLM training [1]</figcaption></figure> <p>Before diving into RL specifics, let’s briefly recap the three main stages of training a Large Language Model:</p> <ul> <li> <strong>Pre-training</strong>: the model is trained on a massive dataset to predict the next token in a sequence based on preceding tokens.</li> <li> <strong>Supervised Fine-Tuning (SFT)</strong>: the model is then <strong>fine-tuned</strong> on more targeted data and aligned with specific instructions.</li> <li> <strong>Reinforcement Learning </strong>(often called <em>RLHF</em> for Reinforcement Learning with Human Feedback): this is the focus of this article. The main goal is to further refine responses’ alignments with human preferences, by allowing the model to learn directly from feedback.</li> </ul> <h3>Reinforcement Learning Basics</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*n2QZe4jUFk_OccaY.png"><figcaption>A robot trying to exit a maze! [2]</figcaption></figure> <p>Before diving deeper, let’s briefly revisit the core ideas behind Reinforcement Learning.</p> <p>RL is quite straightforward to understand at a high level: an<strong> agent</strong> interacts with an <strong>environment</strong>. The agent resides in a specific <strong>state</strong> within the environment and can take <strong>actions</strong> to transition to other states. Each action yields a <strong>reward</strong> from the environment: this is how the environment provides feedback that guides the agent’s future actions.</p> <p>Consider the following example: a <strong>robot</strong> (the agent) navigates (and tries to exit) a <strong>maze</strong> (the environment).</p> <ul> <li>The <strong>state</strong> is the current situation of the environment (the robot’s position in the maze).</li> <li>The robot can take different <strong>actions</strong>: for example, it can move forward, turn left, or turn right.</li> <li>Successfully navigating towards the exit yields a <strong>positive reward</strong>, while hitting a wall or getting stuck in the maze results in <strong>negative rewards.</strong> </li> </ul> <p>Easy! Now, let’s now make an analogy to how RL is used in the context of LLMs.</p> <h3>RL in the context of LLMs</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*8hJsPwyITye84NWqHAe2MA.png"><figcaption>Simplified RLHF Process [3]</figcaption></figure> <p>When used during LLM training, RL is defined by the following components:</p> <ul> <li>The LLM itself <strong>is the agent</strong> </li> <li> <strong>Environment</strong>: everything external to the LLM, including user prompts, feedback systems, and other contextual information. This is basically the framework the LLM is interacting with during training.</li> <li> <strong>Actions: </strong>these are responses to a query from the model. More specifically: these are the <strong>tokens</strong> that the LLM decides to generate in response to a query.</li> <li> <strong>State: </strong>the current query being answered along with tokens the LLM has generated so far (i.e., the partial responses).</li> <li> <strong>Rewards:</strong> this is a bit more tricky here: unlike the maze example above, there is <strong>usually</strong> no binary reward. In the context of LLMs, rewards usually come from a separate <em>reward model</em>, which outputs a score for each (query, response) pair. This model is trained from human-annotated data (hence “RLHF”) where annotators rank different responses. The goal is for higher-quality responses to receive higher rewards.</li> </ul> <blockquote>Note: in some cases, rewards can actually get simpler. For example, in DeepSeekMath, <strong>rule-based approaches</strong> can be used because math responses tend to be more deterministic (correct or wrong answer)</blockquote> <p><strong>Policy</strong> is the final concept we need for now. In RL terms, a policy is simply the strategy for deciding which action to take. In the case of an LLM, the policy outputs a probability distribution over possible tokens at each step: in short, this is what the model uses to sample the next token to generate. Concretely, the policy is determined by the model’s parameters (weights). During RL training, we adjust these parameters so the LLM becomes more likely to produce “better” tokens— that is, tokens that produce higher reward scores.</p> <p>We often write the policy as:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/179/0*oj50mh1PYdcv5CBf.png"></figure> <p>where <em>a</em> is the action (a token to generate), <em>s</em> the state (the query and tokens generated so far), and <em>θ </em>(model’s parameters).</p> <p>This idea of finding the best policy is the whole point of RL! Since we don’t have labeled data (like we do in supervised learning) <strong>we use rewards to adjust our policy to take better actions.</strong> <em>(In LLM terms: we adjust the parameters of our LLM to generate better tokens.)</em></p> <h3>TRPO (Trust Region Policy Optimization)</h3> <h4>An analogy with supervised learning</h4> <p>Let’s take a quick step back to how supervised learning typically works. you have labeled data and use a loss function (like cross-entropy) to measure how close your model’s predictions are to the true labels.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/188/0*YaLNgOyGdTJdnmzK.png"></figure> <p>We can then use algorithms like backpropagation and gradient descent to minimize our loss function and update the weights <em>θ</em> of our model.</p> <p>Recall that our policy also outputs probabilities! In that sense, it is analogous to the model’s predictions in supervised learning… We are tempted to write <em>something like</em>:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/248/0*mCxBSj_yWyLAHEEV.png"></figure> <p>where <em>s</em> is the current state and <em>a</em> is a possible action.</p> <p><em>A(s, a) </em>is called the <strong>advantage function</strong> and measures how good is the chosen action in the current state, compared to a baseline. This is very much like the notion of <strong>labels </strong>in supervised learning but derived from <strong>rewards</strong> instead of explicit labeling. <em>To simplify</em>, we can write the advantage as:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/313/0*Bkfvw7w81TyeuKEs.png"></figure> <p>In practice, the baseline is calculated using a <strong>value function</strong>. This is a common term in RL that I will explain later. What you need to know for now is that it measures the expected reward we would receive if we continue following the current policy from the state <em>s</em>.</p> <h4>What is TRPO?</h4> <p>TRPO (Trust Region Policy Optimization) builds on this idea of using the advantage function but adds a critical ingredient for <strong>stability</strong>: it <strong>constrains</strong> how far the new policy can deviate from the old policy at each update step (similar to what we do with batch gradient descent for example).</p> <ul><li>It introduces a KL divergence term (see it as a measure of similarity) between the current and the old policy:</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/138/0*2pSGRrxLNHmL9Yns.png"></figure> <ul><li>It also divides the policy by the old policy. This ratio, multiplied by the advantage function, gives us a sense of how beneficial each update is <strong>relative to the old policy</strong>.</li></ul> <p>Putting it all together, TRPO tries to <strong>maximize</strong> a surrogate objective (which involves the advantage and the policy ratio) subject to a <strong>KL divergence constraint</strong>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/405/0*O6IZum3z9DVSJTIS.png"></figure> <h3>PPO (Proximal Policy Optimization)</h3> <p>While TRPO was a significant advancement, it’s no longer used widely in practice, especially for training LLMs, due to its computationally intensive gradient calculations.</p> <blockquote>Instead, PPO is now the preferred approach in most LLMs architecture, including ChatGPT, Gemini, and more.</blockquote> <p>It is actually quite similar to TRPO, but instead of enforcing <strong>a hard constraint on the KL divergence</strong>, PPO introduces a “<strong>clipped</strong> surrogate objective” that implicitly restricts policy updates, and greatly simplifies the optimization process.</p> <p>Here is a breakdown of the PPO objective function we maximize to tweak our model’s parameters.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*oVi9NcQvD15nTp4G7rJueA.png"><figcaption>Image by the Author</figcaption></figure> <h3>GRPO (Group Relative Policy Optimization)</h3> <h4>How is the value function usually obtained?</h4> <p>Let’s first talk more about the <strong>advantage</strong> and the <strong>value functions</strong> I introduced earlier.</p> <p>In typical setups (like PPO), a <strong>value model</strong> is trained alongside the policy. Its goal is to predict the value of each action we take (each token generated by the model), using the rewards we obtain (remember that the value should represent the expected cumulative reward).</p> <p>Here is how it works in practice. Take the query “What is 2+2?” as an example. Our model outputs “2+2 is 4” and receives a reward of 0.8 for that response. We then go backward and attribute <strong>discounted rewards</strong> to each prefix:</p> <ul> <li>“2+2 is 4” gets a value of 0.8</li> <li>“2+2 is” (1 token backward) gets a value of 0.8<em>γ</em> </li> <li>“2+2” (2 tokens backward) gets a value of 0.8<em>γ²</em> </li> <li>etc.</li> </ul> <p>where <em>γ</em> is the discount factor (0.9 for example). We then use these prefixes and associated values to train the value model.</p> <blockquote>Important note: the value model and the reward model are two different things. The reward model is trained before the RL process and uses pairs of (query, response) and human ranking. The value model is trained concurrently to the policy, and aims at predicting the future expected reward at each step of the generation process.</blockquote> <h4>What’s new in GRPO</h4> <p>Even if in practice, the reward model is often derived from the policy (training only the “head”), we still end up maintaining many models and handling multiple training procedures (policy, reward, value model). <strong>GRPO</strong> streamlines this by introducing a more efficient method.</p> <p>Remember what I said earlier?</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/313/0*Bkfvw7w81TyeuKEs.png"></figure> <p>In PPO, we decided to use our value function as the baseline. GRPO chooses something else: Here is what GRPO does: concretely, <strong>for each query</strong>, GRPO generates a group of responses (group of size G) and uses their rewards to calculate each response’s advantage as a <strong>z-score</strong>:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/93/0*3cqWAmG6B5tNjU8A.png"></figure> <p>where <em>rᵢ</em> is the reward of the <em>i</em>-th response and <em>μ</em> and <em>σ</em> are the mean and standard deviation of rewards in that group.</p> <p>This naturally eliminates the need for a separate value model. This idea makes a lot of sense when you think about it! <strong>It aligns with the value function we introduced before</strong> and also measures, in a sense, an “expected” reward we can obtain. Also, this new method is well adapted to our problem because LLMs can easily generate multiple <strong>non-deterministic outputs</strong> by using a low <em>temperature </em>(controls the randomness of tokens generation).</p> <blockquote>This is the main idea behind GRPO: getting rid of the value model.</blockquote> <p>Finally, GRPO adds a <strong>KL divergence</strong> term (to be exact, GRPO uses a simple approximation of the KL divergence to improve the algorithm further) directly into its objective, comparing the current policy to a <strong>reference policy</strong> (often the post-SFT model).</p> <p>See the final formulation below:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uw87yMbJbxD2RsV2WWQPrg.png"><figcaption>Image by the Author</figcaption></figure> <p><strong>And… that’s mostly it for GRPO!</strong> I hope this gives you a clear overview of the process: it still relies on the same foundational ideas as TRPO and PPO but introduces additional improvements to make training more efficient, faster, and cheaper — key factors behind <strong>DeepSeek’s success</strong>.</p> <h3>Conclusion</h3> <p>Reinforcement Learning has become a cornerstone for training today’s Large Language Models, particularly through PPO, and more recently GRPO. Each method rests on the same RL fundamentals — states, actions, rewards, and policies — but adds its own twist to balance stability, efficiency, and human alignment:</p> <p>• <strong>TRPO</strong> introduced strict policy constraints via KL divergence</p> <p>• <strong>PPO</strong> eased those constraints with a clipped objective</p> <p>• <strong>GRPO</strong> took an extra step by removing the value model requirement and using group-based reward normalization. Of course, DeepSeek also benefits from other innovations, like high-quality data and other training strategies, but that is for another time!</p> <p>I hope this article gave you a clearer picture of how these methods connect and evolve. I believe that Reinforcement Learning will become <strong>the main focus in training LLMs</strong> to improve their performance, surpassing pre-training and SFT in driving future innovations.</p> <p>If you’re interested in diving deeper, feel free to check out the references below or explore my previous posts.</p> <p>Thanks for reading, and feel free to leave a clap and a comment!</p> <p>Want to learn more about Transformers or dive into the math behind the Curse of Dimensionality? Check out my previous articles:</p> <ul> <li><a href="https://medium.com/towards-data-science/transformers-how-do-they-transform-your-data-72d69e383e0d" rel="external nofollow noopener" target="_blank">Transformers: How Do They Transform Your Data?</a></li> <li><a href="https://medium.com/towards-data-science/the-math-behind-the-curse-of-dimensionality-cf8780307d74" rel="external nofollow noopener" target="_blank">The Math Behind “The Curse of Dimensionality”</a></li> </ul> <ul> <li>Feel free to connect on <a href="https://www.linkedin.com/in/maxime-wolf/" rel="external nofollow noopener" target="_blank">LinkedIn</a> </li> <li>Follow me on <a href="https://github.com/maxime7770" rel="external nofollow noopener" target="_blank">GitHub</a> for more content</li> <li>Visit my website: <a href="http://maximewolf.com/" rel="external nofollow noopener" target="_blank">maximewolf.com</a> </li> </ul> <p>References:</p> <ul> <li>[1] “Foundations of Large Language Models”, 2025. <a href="https://arxiv.org/pdf/2501.09223" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2501.09223</a> </li> <li>[2] <strong>“</strong>Reinforcement Learning<strong>.”</strong> Enaris. Available at: <a href="https://enaris.org/material/en/Reinforcement%20Learning/index.html" rel="external nofollow noopener" target="_blank">https://enaris.org/material/en/Reinforcement%20Learning/index.html</a> </li> <li>[3] Y. Gokhale. “Introduction to LLMs and the Generative AI Part 5: RLHF,” <em>Medium</em>, 2023. Available at: <a href="https://medium.com/@yash9439/introduction-to-llms-and-the-generative-ai-part-5-rlhf-64e83fbcd795" rel="external nofollow noopener" target="_blank">https://medium.com/@yash9439/introduction-to-llms-and-the-generative-ai-part-5-rlhf-64e83fbcd795</a> </li> <li>[4] L. Weng. “An Overview of Reinforcement Learning,” 2018. Available at: <a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/" rel="external nofollow noopener" target="_blank">https://lilianweng.github.io/posts/2018-02-19-rl-overview/</a> </li> <li>[5] “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning”, 2025. <a href="https://arxiv.org/pdf/2501.12948" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2501.12948</a> </li> <li>[6] “DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models”, 2025. <a href="https://arxiv.org/pdf/2402.03300" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2402.03300</a> </li> <li>[7] “Trust Region Policy Optimization”, 2017. <a href="https://arxiv.org/pdf/1502.05477" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/1502.05477</a> </li> </ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f3607a126194" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science-collective/training-large-language-models-from-trpo-to-grpo-f3607a126194" rel="external nofollow noopener" target="_blank">Training Large Language Models: From TRPO to GRPO</a> was originally published in <a href="https://medium.com/data-science-collective" rel="external nofollow noopener" target="_blank">Data Science Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>