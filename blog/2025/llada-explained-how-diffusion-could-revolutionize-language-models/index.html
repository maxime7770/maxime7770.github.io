<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3><strong>LLaDA: The Diffusion Model That Could Redefine Language Generation</strong></h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*feSfRRh6hoH1MXvxkYjKMA.png"><figcaption>Image generated by Dall-E</figcaption></figure> <h3>Introduction</h3> <p>What if we could make language models think<strong> more like humans</strong>? Instead of writing one word at a time, what if they could sketch out their thoughts first, and gradually refine them?</p> <p>This is exactly what Large Language Diffusion Models (LLaDA) introduces: a different approach to current text generation used in Large Language Models (LLMs). Unlike traditional autoregressive models (ARMs), which predict text sequentially, left to right, <strong>LLaDA leverages a diffusion-like process to generate text.</strong> Instead of generating tokens sequentially, it <strong>progressively refines masked text until it forms a coherent response</strong>.</p> <p>In this article, we will dive into how LLaDA works, why it matters, and how it could shape the next generation of LLMs.</p> <p>I hope you enjoy the article!</p> <h3>The current state of LLMs</h3> <p>To appreciate the innovation that LLaDA represents, we first need to understand how current large language models (LLMs) operate. Modern LLMs follow a two-step training process that has become an industry standard:</p> <ol> <li> <strong>Pre-training</strong>: The model learns general language patterns and knowledge by predicting the next token in massive text datasets through self-supervised learning.</li> <li> <strong>Supervised Fine-Tuning (SFT)</strong>: The model is refined on carefully curated data to improve its ability to follow instructions and generate useful outputs.</li> </ol> <p><em>Note that current LLMs often use RLHF as well to further refine the weights of the model, but this is not used by LLaDA so we will skip this step here.</em></p> <p>These models, primarily based on the Transformer architecture, generate text <strong>one token at a time</strong> using next-token prediction.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*F6HkX1GkSF5WYOwZ4Afo_w.png"><figcaption>Simplified Transformer architecture for text generation (Image by the author)</figcaption></figure> <p>Here is a simplified illustration of how data passes through such a model. <strong>Each token is embedded into a vector and is transformed through successive transformer layers</strong>. In current LLMs (LLaMA, ChatGPT, DeepSeek, etc), a classification head is used only on the last token embedding to predict the next token in the sequence.</p> <p>This works thanks to the concept of <strong>masked self-attention</strong>: each token attends to all the tokens that come before it. We will see later how LLaDA can get rid of the mask in its attention layers.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/899/1*zVrq7u6Wo-i1pclptiU_xQ.png"><figcaption>Attention process: input embeddings are multiplied byQuery, Key, and Value matrices to generate new embeddings (Image by the author, inspired by [3])</figcaption></figure> <p><em>If you want to learn more about Transformers, check out my Medium article below:</em></p> <p><a href="https://medium.com/towards-data-science/transformers-how-do-they-transform-your-data-72d69e383e0d" rel="external nofollow noopener" target="_blank">Transformers: How Do They Transform Your Data?</a></p> <p>While this approach has led to impressive results, it also comes with significant limitations, some of which have motivated the development of LLaDA.</p> <h3>Current limitations of LLMs</h3> <p>Current LLMs face several critical challenges:</p> <h4>Computational Inefficiency</h4> <p>Imagine having to write a novel where <strong>you can only think about one word at a time,</strong> and for each word, you need to reread everything you’ve written so far. This is essentially how current LLMs operate — they predict one token at a time, requiring a complete processing of the previous sequence for each new token. Even with optimization techniques like KV caching, this process is <strong>quite computationally expensive and time-consuming</strong>.</p> <h4>Limited Bidirectional Reasoning</h4> <p>Traditional autoregressive models (ARMs) are like writers who could never look ahead or revise what they’ve written so far. <strong>They can only predict future tokens based on past ones, which limits their ability to reason about relationships between different parts of the text.</strong> As humans, we often have a general idea of what we want to say before writing it down, current LLMs lack this capability in some sense.</p> <h4>Amount of data</h4> <p>Existing models require <strong>enormous amounts of training data</strong> to achieve good performance, making them resource-intensive to develop and potentially limiting their applicability in specialized domains with limited data availability.</p> <h3>What is LLaDA</h3> <p>LLaDA introduces a fundamentally different approach to language generation by replacing traditional autoregression with a <strong>“diffusion-based”</strong> process (we will dive later into why this is called “diffusion”).</p> <p>Let’s understand how this works, step by step, starting with pre-training.</p> <h4>LLaDA pre-training</h4> <p>Remember that we don’t need any “labeled” data during the pre-training phase. The objective is to feed a very large amount of raw text data into the model. For each text sequence, we do the following:</p> <ol> <li>We fix a maximum length (similar to ARMs). Typically, this could be 4096 tokens. 1% of the time, the lengths of sequences are randomly sampled between 1 and 4096 and padded so that the model is also exposed to shorter sequences.</li> <li>We randomly choose a “masking rate”. For example, one could pick 40%.</li> <li>We mask each token with a probability of 0.4. What does “masking” mean exactly? Well, we simply replace the token <strong>with a special token</strong>: <strong>&lt;MASK&gt;</strong>. As with any other token, this token is associated with a particular index and embedding vector that the model can process and interpret during training.</li> <li>We then feed our entire sequence into our transformer-based model. This process transforms all the input embedding vectors into new embeddings. We <strong>apply the classification head to each of the masked tokens</strong> to get a prediction for each. Mathematically, our loss function averages cross-entropy losses over all the masked tokens in the sequence, as below:</li> </ol> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*_DFxLzcQ9fskIlIymfuLBA.png"><figcaption>Loss function used for LLaDA (Image by the author)</figcaption></figure> <p>5. And… we repeat this procedure for billions or trillions of text sequences.</p> <blockquote><strong><em>Note, that unlike ARMs, LLaDA can fully utilize bidirectional dependencies in the text: it doesn’t require masking in attention layers anymore. However, this can come at an increased computational cost.</em></strong></blockquote> <p>Hopefully, you can see how the training phase itself (the flow of the data into the model) is very similar to any other LLMs. <strong>We simply predict randomly masked tokens instead of predicting what comes next.</strong></p> <h4>LLaDA SFT</h4> <p>For <strong>auto-regressive models</strong>, SFT is very similar to pre-training, except that we have pairs of <em>(prompt, response) </em>and want to generate the response when giving the prompt as input.</p> <p>This is exactly the <strong>same concept for LLaDA</strong>! Mimicking the pre-training process: we simply pass the prompt and the response, mask random tokens <strong>from the response only</strong>, and feed the full sequence into the model, which <strong>will predict missing tokens from the response</strong>.</p> <h4>The innovation in inference</h4> <p>Innovation is where LLaDA gets more interesting, and truly utilizes the “diffusion” paradigm.</p> <p>Until now, we always randomly masked some text as input and asked the model to predict these tokens. But during inference, <strong>we only have access to the prompt </strong>and we need to generate the entire response. You might think (and it’s not wrong), that the model has seen examples where the masking rate was very high (potentially 1) during SFT, and it had to learn, somehow, how to <strong>generate a full response from a prompt</strong>.</p> <p>However, generating the full response at once during inference will likely produce very poor results because the model lacks information. Instead, we need a method to <strong>progressively refine predictions</strong>, and that’s where the key idea of <strong>‘remasking’ </strong>comes in.</p> <p>Here is how it works, at each step of the text generation process:</p> <ul> <li>Feed the current input to the model (this is the prompt, followed by <strong>&lt;MASK&gt;</strong> tokens)</li> <li>The model generates one embedding for each input token. We get predictions for the <strong>&lt;MASK&gt;</strong> tokens only. And here is the important step: <strong>we remask a portion of them</strong>. In particular: we only keep the “best” tokens i.e. the ones <strong>with the best predictions</strong>, with the highest confidence.</li> <li>We can use this <strong>partially unmasked sequence</strong> as input in the next generation step and repeat until all tokens are unmasked.</li> </ul> <p>You can see that, interestingly, <strong>we have much more control</strong> over the generation process compared to ARMs: we could choose to remask 0 tokens (only one generation step), or we could decide to keep only the best token every time (as many steps as tokens in the response). Obviously, <strong>there is a trade-off here between the quality of the predictions and inference time.</strong></p> <p>Let’s illustrate that with a simple example (in that case, I choose to keep the best 2 tokens at every step)</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Jfxxelqbjz8u-QBLeXZYzA.png"><figcaption>LLaDA generation process example (Image by the author)</figcaption></figure> <p>Note, in practice, the remasking step would work as follows. Instead of remasking a fixed number of tokens, <strong>we would remask a proportion of s/t tokens over time, from t=1 down to 0, where s is in [0, t].</strong> In particular, this means we remask fewer and fewer tokens as the number of generation steps increases.</p> <p><em>Example</em>: if we want N sampling steps (so N discrete steps from t=1 down to t=1/N with steps of 1/N), taking s = (t-1/N) is a good choice, and ensures that s=0 at the end of the process.</p> <p>The image below summarizes the 3 steps described above. “Mask predictor” simply denotes the LLM (LLaDA), predicting masked tokens.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*00SyWDItOWMv0bUdKTye9A.png"><figcaption>Pre-training (a.), SFT (b.) and inference (c.) using LLaDA. (source: [1])</figcaption></figure> <h3>Can autoregression and diffusion be combined?</h3> <p>Another clever idea developed in LLaDA is <strong>to combine diffusion with traditional autoregressive generation to use the best of both worlds!</strong> This is called <strong>semi-autoregressive diffusion</strong>.</p> <ul> <li>Divide the generation process into blocks (for instance, 32 tokens in each block).</li> <li>The objective is to <strong>generate one block at a time</strong> (like we would generate one token at a time in ARMs).</li> <li> <strong>For each block, we apply the diffusion logic</strong> by progressively unmasking tokens to reveal the entire block. Then move on to predicting the next block.</li> </ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zaZOcBVb5G4sqY60J68ulg.png"><figcaption>Semi-autoregressive process (source: [1])</figcaption></figure> <p>This is a hybrid approach: we probably lose some of the “backward” generation and parallelization capabilities of the model, but <strong>we better “guide” the model</strong> towards the final output.</p> <p>I think this is a very interesting idea because it depends a lot on a hyperparameter (the number of blocks), that can be tuned. I imagine different tasks might benefit more from the backward generation process, while others might benefit more from the more “guided” generation from left to right (more on that in the last paragraph).</p> <h3>Why “Diffusion”?</h3> <p>I think it’s important to briefly explain where this term actually comes from. It reflects a similarity with <strong>image diffusion models (like Dall-E)</strong>, which have been very popular for image generation tasks.</p> <p>In image diffusion, a model first adds noise to an image until it’s unrecognizable, then learns to reconstruct it step by step. LLaDA applies this idea to text <strong>by masking tokens instead of adding noise</strong>, and then progressively <strong>unmasking them</strong> to generate coherent language. In the context of image generation, the masking step is often called “noise scheduling”, and the reverse (remasking) is the “denoising” step.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*y1hRHhA3CaJkadBh"><figcaption>How do Diffusion Models work? (source: [2])</figcaption></figure> <p>You can also see LLaDA as some type of <strong>discrete</strong> (non-continuous) diffusion model: we don’t add noise to tokens, but we “deactivate” some tokens by masking them, and the model learns how to unmask a portion of them.</p> <h3>Results</h3> <p>Let’s go through <em>a few </em>of the interesting results of LLaDA.</p> <p><em>You can find all the results in the paper. I chose to focus on what I find the most interesting here.</em></p> <ul> <li> <strong>Training efficiency</strong>: LLaDA shows similar performance to ARMs with the same number of parameters, but u<strong>ses much fewer tokens</strong> during training (and no RLHF)! For example, the 8B version uses around 2.3T tokens, compared to 15T for LLaMa3.</li> <li> <strong>Using different block and answer lengths for different tasks</strong>: for example, the block length is particularly large for the Math dataset, and the model demonstrates strong performance for this domain. This could suggest that mathematical reasoning may benefit more from the <strong>diffusion-based and backward process</strong>.</li> </ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/591/1*npIPih_7TGBiT6bEGnYfzA.png"><figcaption>Source: [1]</figcaption></figure> <ul><li>Interestingly, LLaDA does better on the “Reversal poem completion task”. This task requires the model to <strong>complete a poem in reverse order</strong>, starting from the last lines and working backward. As expected, ARMs struggle due to their strict left-to-right generation process.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/475/1*rO3FqBzTLRoXCElD0LxGfw.png"><figcaption>Source: [1]</figcaption></figure> <blockquote>LLaDA is not just an experimental alternative to ARMs: it shows real advantages in efficiency, structured reasoning, and bidirectional text generation.</blockquote> <h3>Conclusion</h3> <p>I think LLaDA is a promising approach to language generation. Its ability to generate multiple tokens in parallel while maintaining global coherence could definitely lead to <strong>more efficient training</strong>, <strong>better reasoning</strong>, and <strong>improved context understanding</strong> with fewer computational resources.</p> <p>Beyond efficiency, I think LLaDA also brings a lot of <strong>flexibility</strong>. By adjusting parameters like the number of blocks generated, and the number of generation steps, it can <strong>better adapt to different tasks and constraints</strong>, making it a versatile tool for various language modeling needs, and allowing <strong>more human control</strong>. Diffusion models could also play an important role in pro-active AI and agentic systems by being able to reason more holistically.</p> <p>As research into diffusion-based language models advances, LLaDA could become a useful step toward <strong>more natural and efficient language models</strong>. While it’s still early, I believe this shift from sequential to parallel generation is an interesting direction for AI development.</p> <p>Thanks for reading!</p> <p>Check out my previous articles:</p> <ul> <li><a href="https://medium.com/data-science-collective/training-large-language-models-from-trpo-to-grpo-f3607a126194" rel="external nofollow noopener" target="_blank">Training Large Language Models: From TRPO to GRPO</a></li> <li><a href="https://medium.com/towards-data-science/the-math-behind-the-curse-of-dimensionality-cf8780307d74" rel="external nofollow noopener" target="_blank">The Math Behind “The Curse of Dimensionality”</a></li> </ul> <ul> <li>Feel free to connect on <a href="https://www.linkedin.com/in/maxime-wolf/" rel="external nofollow noopener" target="_blank">LinkedIn</a> </li> <li>Follow me on <a href="https://github.com/maxime7770" rel="external nofollow noopener" target="_blank">GitHub</a> for more content</li> <li>Visit my website: <a href="http://maximewolf.com/" rel="external nofollow noopener" target="_blank">maximewolf.com</a> </li> </ul> <h4>References:</h4> <ul> <li>[1] Liu, C., Wu, J., Xu, Y., Zhang, Y., Zhu, X., &amp; Song, D. (2024). <strong>Large Language Diffusion Models</strong>. <em>arXiv preprint arXiv:2502.09992</em>. <a href="https://arxiv.org/pdf/2502.09992" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2502.09992</a> </li> <li>[2] <a href="https://dl.acm.org/doi/10.1145/3626235" rel="external nofollow noopener" target="_blank">Yang, Ling, et al. “Diffusion models: A comprehensive survey of methods and applications.” ACM Computing Surveys 56.4 (2023): 1–39.</a> </li> <li>[3] Alammar, J. (2018, June 27). <strong>The Illustrated Transformer</strong>. <em>Jay Alammar’s Blog</em>. <a href="https://jalammar.github.io/illustrated-transformer/" rel="external nofollow noopener" target="_blank">https://jalammar.github.io/illustrated-transformer/</a> </li> </ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=950bcce4ec09" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science-collective/llada-explained-how-diffusion-could-revolutionize-language-models-950bcce4ec09" rel="external nofollow noopener" target="_blank">LLaDA Explained: How Diffusion Could Revolutionize Language Models</a> was originally published in <a href="https://medium.com/data-science-collective" rel="external nofollow noopener" target="_blank">Data Science Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>