<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body>
<h4>Diving into the Transformers architecture and what makes them unbeatable at language¬†tasks</h4>
<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*eBqke1in-L9fFZrZ6hjdRA.jpeg"><figcaption>Image by the¬†author</figcaption></figure>
<p>In the rapidly evolving landscape of artificial intelligence and machine learning, one innovation stands out for its profound impact on how we process, understand, and generate data: <strong>Transformers</strong>. Transformers have revolutionized the field of natural language processing (NLP) and beyond, powering some of today‚Äôs most advanced AI applications. But what exactly are Transformers, and how do they manage to transform data in such groundbreaking ways? This article demystifies the inner workings of Transformer models, focusing on the <strong>encoder architecture</strong>. We will start by going through the implementation of a Transformer encoder in Python, breaking down its main components. Then, we will visualize how Transformers process and adapt input data during training.</p>
<p>While this blog doesn‚Äôt cover every architectural detail, it provides an implementation and an overall understanding of the transformative power of Transformers. For an in-depth explanation of Transformers, I suggest you look at the excellent Stanford CS224-n¬†course.</p>
<p>I also recommend following the <a href="https://github.com/maxime7770/Transformers-Insights" rel="external nofollow noopener" target="_blank">GitHub repository</a> associated with this article for additional details.¬†üòä</p>
<h3>What is a Transformer encoder architecture?</h3>
<figure><img alt="" src="https://cdn-images-1.medium.com/max/982/1*ittgNWKSm6uejPpyGu2SdQ.png"><figcaption>The Transformer model from <a href="https://arxiv.org/pdf/1706.03762.pdf" rel="external nofollow noopener" target="_blank">Attention Is All You¬†Need</a></figcaption></figure>
<p>This picture shows the original Transformer architecture, combining an encoder and a decoder for sequence-to-sequence language¬†tasks.</p>
<p>In this article, we will focus on the encoder architecture (the red block on the picture). This is what the popular BERT model is using under the hood: the primary focus is on <strong>understanding and representing the data</strong>, rather than generating sequences. It can be used for a variety of applications: text classification, named-entity recognition (NER), extractive question answering, etc.</p>
<p>So, how is the data actually transformed by this architecture? We will explain each component in detail, but here is an overview of the¬†process.</p>
<ul>
<li>The input text is <strong>tokenized</strong>: the Python string is transformed into a list of tokens (numbers)</li>
<li>Each token is passed through an <strong>Embedding layer</strong> that outputs a vector representation for each¬†token</li>
<li>The embeddings are then further encoded with a <strong>Positional Encoding layer</strong>, adding information about the position of each token in the¬†sequence</li>
<li>These new embeddings are transformed by a series of <strong>Encoder Layers</strong>, using a self-attention mechanism</li>
<li>A <strong>task-specific head</strong> can be added. For example, we will later use a classification head to classify movie reviews as positive or¬†negative</li>
</ul>
<p>That is important to understand that the Transformer architecture transforms the embedding vectors by mapping them from one representation in a high-dimensional space to another within the same space, applying a series of complex transformations.</p>
<h3>Implementing an encoder architecture in¬†Python</h3>
<h4>The Positional Encoder¬†layer</h4>
<p>Unlike RNN models, the attention mechanism makes no use of the order of the input sequence. The PositionalEncoder class adds positional encodings to the input embeddings, using two mathematical functions: cosine and¬†sine.</p>
<figure><img alt="" src="https://cdn-images-1.medium.com/max/576/1*971n-dZ3KwprUN0GKG-_dg.png"><figcaption>Positional encoding matrix definition from <a href="https://arxiv.org/pdf/1706.03762.pdf" rel="external nofollow noopener" target="_blank">Attention Is All You¬†Need</a></figcaption></figure>
<p>Note that positional encodings don‚Äôt contain trainable parameters: there are the results of deterministic computations, which makes this method very tractable. Also, sine and cosine functions take values between -1 and 1 and have useful periodicity properties to help the model learn patterns about the <strong>relative positions of¬†words</strong>.</p>
<pre>class PositionalEncoder(nn.Module):<br>    def __init__(self, d_model, max_length):<br>        super(PositionalEncoder, self).__init__()<br>        self.d_model = d_model<br>        self.max_length = max_length<br>        <br>        # Initialize the positional encoding matrix<br>        pe = torch.zeros(max_length, d_model)<br><br>        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)<br>        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))<br>        <br>        # Calculate and assign position encodings to the matrix<br>        pe[:, 0::2] = torch.sin(position * div_term)<br>        pe[:, 1::2] = torch.cos(position * div_term)<br>        self.pe = pe.unsqueeze(0)<br>    <br>    def forward(self, x):<br>        x = x + self.pe[:, :x.size(1)] # update embeddings<br>        return x</pre>
<h4>Multi-Head Self-Attention</h4>
<p>The self-attention mechanism is the key component of the encoder architecture. Let‚Äôs ignore the ‚Äúmulti-head‚Äù for now. Attention is a way to determine for each token (i.e. each embedding) the <strong>relevance of all other embeddings to that token</strong>, to obtain a more refined and contextually relevant encoding.</p>
<figure><img alt="" src="https://cdn-images-1.medium.com/max/609/1*FtvndpgqfD0GHd1ocOlxTA.jpeg"><figcaption>How does‚Äúit‚Äù pay attention to other words of the sequence? (<a href="https://jalammar.github.io/illustrated-transformer/" rel="external nofollow noopener" target="_blank">The Illustrated Transformer</a>)</figcaption></figure>
<p>There are 3 steps in the self-attention mechanism.</p>
<ul>
<li>Use matrices Q, K, and V to respectively transform the inputs ‚Äú<strong>query</strong>‚Äù, ‚Äú<strong>key</strong>‚Äù and ‚Äú<strong>value</strong>‚Äù. Note that for self-attention, query, key, and values are all equal to our input embedding</li>
<li>Compute the attention score using cosine similarity (a dot product) between the <strong>query</strong> and the <strong>key</strong>. Scores are scaled by the square root of the embedding dimension to stabilize the gradients during¬†training</li>
<li>Use a softmax layer to make these scores <strong>probabilities</strong>
</li>
<li>The output is the weighted average of the <strong>values</strong>, using the attention scores as the¬†weights</li>
</ul>
<p>Mathematically, this corresponds to the following formula.</p>
<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*epJS1mkxjNqAUS0rjJhkAQ.png"><figcaption>The Attention Mechanism from <a href="https://arxiv.org/pdf/1706.03762.pdf" rel="external nofollow noopener" target="_blank">Attention Is All You¬†Need</a></figcaption></figure>
<p>What does ‚Äúmulti-head‚Äù mean? Basically, we can apply the described self-attention mechanism process several times, in parallel, and concatenate and project the outputs. This allows each head to f<strong>ocus on different semantic aspects of the sentence</strong>.</p>
<p>We start by defining the number of heads, the dimension of the embeddings (d_model), and the dimension of each head (head_dim). We also initialize the Q, K, and V matrices (linear layers), and the final projection layer.</p>
<pre>class MultiHeadAttention(nn.Module):<br>    def __init__(self, d_model, num_heads):<br>        super(MultiHeadAttention, self).__init__()<br>        self.num_heads = num_heads<br>        self.d_model = d_model<br>        self.head_dim = d_model // num_heads<br><br>        self.query_linear = nn.Linear(d_model, d_model)<br>        self.key_linear = nn.Linear(d_model, d_model)<br>        self.value_linear = nn.Linear(d_model, d_model)      <br>        self.output_linear = nn.Linear(d_model, d_model)</pre>
<p>When using multi-head attention, we apply each attention head with a reduced dimension (head_dim instead of d_model) as in the original paper, making the total computational cost similar to a one-head attention layer with full dimensionality. Note this is a logical split only. What makes multi-attention so powerful is it can still be represented via a single matrix operation, making computations very efficient on¬†GPUs.</p>
<pre>def split_heads(self, x, batch_size):<br>        # Split the sequence embeddings in x across the attention heads<br>        x = x.view(batch_size, -1, self.num_heads, self.head_dim)<br>        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)</pre>
<p>We compute the attention scores and use a mask to avoid using attention on padded tokens. We apply a softmax activation to make these scores probabilities.</p>
<pre>def compute_attention(self, query, key, mask=None):<br>      # Compute dot-product attention scores<br>      # dimensions of query and key are (batch_size * num_heads, seq_length, head_dim)<br>      scores = query @ key.transpose(-2, -1) / math.sqrt(self.head_dim)<br>      # Now, dimensions of scores is (batch_size * num_heads, seq_length, seq_length)<br>      if mask is not None:<br>          scores = scores.view(-1, scores.shape[0] // self.num_heads, mask.shape[1], mask.shape[2]) # for compatibility<br>          scores = scores.masked_fill(mask == 0, float('-1e20')) # mask to avoid attention on padding tokens<br>          scores = scores.view(-1, mask.shape[1], mask.shape[2]) # reshape back to original shape<br>      # Normalize attention scores into attention weights<br>      attention_weights = F.softmax(scores, dim=-1)<br><br>      return attention_weights</pre>
<p>The forward attribute performs the multi-head logical split and computes the attention weights. Then, we get the output by multiplying these weights by the values. Finally, we reshape the output and project it with a linear¬†layer.</p>
<pre>def forward(self, query, key, value, mask=None):<br>      batch_size = query.size(0)<br><br>      query = self.split_heads(self.query_linear(query), batch_size)<br>      key = self.split_heads(self.key_linear(key), batch_size)<br>      value = self.split_heads(self.value_linear(value), batch_size)<br><br>      attention_weights = self.compute_attention(query, key, mask)<br>          <br>      # Multiply attention weights by values, concatenate and linearly project outputs<br>      output = torch.matmul(attention_weights, value)<br>      output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)<br>      return self.output_linear(output)</pre>
<h4>The Encoder¬†Layer</h4>
<p>This is the main component of the architecture, which leverages multi-head self-attention. We first implement a simple class to perform a feed-forward operation through 2 dense¬†layers.</p>
<pre>class FeedForwardSubLayer(nn.Module):<br>    def __init__(self, d_model, d_ff):<br>        super(FeedForwardSubLayer, self).__init__()<br>        self.fc1 = nn.Linear(d_model, d_ff)<br>        self.fc2 = nn.Linear(d_ff, d_model)<br>        self.relu = nn.ReLU()<br><br>    def forward(self, x):<br>        return self.fc2(self.relu(self.fc1(x)))</pre>
<p>We can now code the logic for the encoder layer. We start by applying self-attention to the input, which gives a vector of the same dimension. We then use our mini feed-forward network with Layer Norm layers. Note that we also use skip connections before applying normalization.</p>
<pre>class EncoderLayer(nn.Module):<br>    def __init__(self, d_model, num_heads, d_ff, dropout):<br>        super(EncoderLayer, self).__init__()<br>        self.self_attn = MultiHeadAttention(d_model, num_heads)<br>        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)<br>        self.norm1 = nn.LayerNorm(d_model)<br>        self.norm2 = nn.LayerNorm(d_model)<br>        self.dropout = nn.Dropout(dropout)<br><br>    def forward(self, x, mask):<br>        attn_output = self.self_attn(x, x, x, mask)<br>        x = self.norm1(x + self.dropout(attn_output)) # skip connection and normalization<br>        ff_output = self.feed_forward(x)<br>        return self.norm2(x + self.dropout(ff_output)) # skip connection and normalization</pre>
<h4>Putting Everything Together</h4>
<p>It‚Äôs time to create our final model. We pass our data through an embedding layer. This transforms our raw tokens (integers) into a numerical vector. We then apply our positional encoder and several (num_layers) encoder¬†layers.</p>
<pre>class TransformerEncoder(nn.Module):<br>    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):<br>        super(TransformerEncoder, self).__init__()<br>        self.embedding = nn.Embedding(vocab_size, d_model)<br>        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)<br>        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])<br>    <br>    def forward(self, x, mask):<br>        x = self.embedding(x)<br>        x = self.positional_encoding(x)<br>        for layer in self.layers:<br>            x = layer(x, mask)<br>        return x</pre>
<p>We also create a ClassifierHead class which is used to transform the final embedding into class probabilities for our classification task.</p>
<pre>class ClassifierHead(nn.Module):<br>    def __init__(self, d_model, num_classes):<br>        super(ClassifierHead, self).__init__()<br>        self.fc = nn.Linear(d_model, num_classes)<br><br>    def forward(self, x):<br>        logits = self.fc(x[:, 0, :]) # first token corresponds to the classification token<br>        return F.softmax(logits, dim=-1)</pre>
<p>Note that the dense and softmax layers are only applied on the first embedding (corresponding to the first token of our input sequence). This is because when tokenizing the text, the first token is the [CLS] token which stands for ‚Äúclassification.‚Äù The [CLS] token is designed to aggregate the entire sequence‚Äôs information into a single embedding vector, serving as a summary representation that can be used for classification tasks.</p>
<p>Note: the concept of including a [CLS] token originates from BERT, which was initially trained on tasks like next-sentence prediction. The [CLS] token was inserted to predict the likelihood that sentence B follows sentence A, with a [SEP] token separating the 2 sentences. For our model, the [SEP] token simply marks the end of the input sentence, as shown¬†below.</p>
<figure><img alt="" src="https://cdn-images-1.medium.com/max/354/1*t4iX4ZPWJmnj42AIlTpvoA.png"><figcaption>[CLS] Token in BERT Architecture (<a href="https://seunghan96.github.io/dl/nlp/28.-nlp-BERT-%EC%9D%B4%EB%A1%A0/" rel="external nofollow noopener" target="_blank">All About¬†AI</a>)</figcaption></figure>
<p>When you think about it, it‚Äôs really mind-blowing that this single [CLS] embedding is able to capture so much information about the entire sequence, thanks to the self-attention mechanism‚Äôs ability to weigh and synthesize the importance of every piece of the text in relation to each¬†other.</p>
<h3>Training and visualization</h3>
<p>Hopefully, the previous section gives you a better understanding of how our Transformer model transforms the input data. We will now write our training pipeline for our binary classification task using the IMDB dataset (movie reviews). Then, we will visualize the embedding of the [CLS] token during the training process to see how our model transformed it.</p>
<p>We first define our hyperparameters, as well as a BERT tokenizer. In the GitHub repository, you can see that I also coded a function to select a subset of the dataset with only 1200 train and 200 test examples.</p>
<pre>num_classes = 2 # binary classification<br>d_model = 256 # dimension of the embedding vectors<br>num_heads = 4 # number of heads for self-attention<br>num_layers = 4 # number of encoder layers<br>d_ff = 512. # dimension of the dense layers in the encoder layers<br>sequence_length = 256 # maximum sequence length <br>dropout = 0.4 # dropout to avoid overfitting<br>num_epochs = 20<br>batch_size = 32<br><br>loss_function = torch.nn.CrossEntropyLoss()<br><br>dataset = load_dataset("imdb")<br>dataset = balance_and_create_dataset(dataset, 1200, 200) # check GitHub repo<br><br>tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', model_max_length=sequence_length)</pre>
<p>You can try to use the BERT tokenizer on one of the sentences:</p>
<pre>print(tokenized_datasets['train']['input_ids'][0])</pre>
<p>Every sequence should start with the token 101, corresponding to [CLS], followed by some non-zero integers and padded with zeros if the sequence length is smaller than 256. Note that these zeros are ignored during the self-attention computation using our¬†‚Äúmask‚Äù.</p>
<pre>tokenized_datasets = dataset.map(encode_examples, batched=True)<br>tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])<br><br>train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=batch_size, shuffle=True)<br>test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=batch_size, shuffle=True)<br><br>vocab_size = tokenizer.vocab_size<br><br>encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)<br>classifier = ClassifierHead(d_model, num_classes)<br><br>optimizer = torch.optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=1e-4)</pre>
<p>We can now write our train function:</p>
<pre>def train(dataloader, encoder, classifier, optimizer, loss_function, num_epochs):<br>    for epoch in range(num_epochs):        <br>        # Collect and store embeddings before each epoch starts for visualization purposes (check repo)<br>        all_embeddings, all_labels = collect_embeddings(encoder, dataloader)<br>        reduced_embeddings = visualize_embeddings(all_embeddings, all_labels, epoch, show=False)<br>        dic_embeddings[epoch] = [reduced_embeddings, all_labels]<br>        <br>        encoder.train()<br>        classifier.train()<br>        correct_predictions = 0<br>        total_predictions = 0<br>        for batch in tqdm(dataloader, desc="Training"):<br>            input_ids = batch['input_ids']<br>            attention_mask = batch['attention_mask'] # indicate where padded tokens are<br>            # These 2 lines make the attention_mask a matrix instead of a vector<br>            attention_mask = attention_mask.unsqueeze(-1)<br>            attention_mask = attention_mask &amp; attention_mask.transpose(1, 2) <br>            labels = batch['label']<br>            optimizer.zero_grad()<br>            output = encoder(input_ids, attention_mask)<br>            classification = classifier(output)<br>            loss = loss_function(classification, labels)<br>            loss.backward()<br>            optimizer.step()<br>            preds = torch.argmax(classification, dim=1)<br>            correct_predictions += torch.sum(preds == labels).item()<br>            total_predictions += labels.size(0)<br>        <br>        epoch_accuracy = correct_predictions / total_predictions<br>        print(f'Epoch {epoch} Training Accuracy: {epoch_accuracy:.4f}')</pre>
<p>You can find the collect_embeddings and visualize_embeddings functions in the GitHub repo. They store the [CLS] token embedding for each sentence of the training set, apply a dimensionality reduction technique called t-SNE to make them 2D vectors (instead of 256-dimensional vectors), and save an animated¬†plot.</p>
<p>Let‚Äôs visualize the¬†results.</p>
<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*K3bwH03hjtra_QQyY5vYGg.gif"><figcaption>Projected [CLS] embeddings for each training point (blue corresponds to positive sentences, red corresponds to negative sentences)</figcaption></figure>
<p>Observing the plot of projected [CLS] embeddings for each training point, we can see the clear distinction between positive (blue) and negative (red) sentences after a few epochs. This visual shows the remarkable capability of the Transformer architecture to adapt embeddings over time and highlights the power of the self-attention mechanism. The data is transformed in such a way that embeddings for each class are well separated, thereby significantly simplifying the task for the classifier head.</p>
<h3>Conclusion</h3>
<p>As we conclude our exploration of the Transformer architecture, it‚Äôs evident that these models are adept at tailoring data to a given task. With the use of positional encoding and multi-head self-attention, Transformers go beyond mere data processing: they interpret and understand information with a level of sophistication previously unseen. The ability to dynamically weigh the relevance of different parts of the input data allows for a more nuanced understanding and representation of the input text. This enhances performance across a wide array of downstream tasks, including text classification, question answering, named entity recognition, and¬†more.</p>
<p>Now that you have a better understanding of the encoder architecture, you are ready to delve into decoder and encoder-decoder models, which are very similar to what we have just explored. Decoders play a pivotal role in generative tasks and are at the core of the popular GPT¬†models.</p>
<ul>
<li>Feel free to connect on¬†<a href="https://www.linkedin.com/in/maxime-wolf/" rel="external nofollow noopener" target="_blank">LinkedIn</a>
</li>
<li>Follow me on <a href="https://github.com/maxime7770" rel="external nofollow noopener" target="_blank">GitHub</a> for more¬†content</li>
<li>Visit my website: <a href="http://maximewolf.com" rel="external nofollow noopener" target="_blank">maximewolf.com</a>
</li>
</ul>
<p><strong>References</strong></p>
<p>[1] Vaswani, Ashish, et al. ‚ÄúAttention Is All You Need.‚Äù <em>31st Conference on Neural Information Processing Systems (NIPS 2017)</em>, Long Beach, CA,¬†USA.</p>
<p>[2] ‚ÄúThe Illustrated Transformer.‚Äù <em>Jay Alammar‚Äôs Blog</em>, June 2018, <a href="http://jalammar.github.io/illustrated-transformer/" rel="external nofollow noopener" target="_blank">http://jalammar.github.io/illustrated-transformer/</a></p>
<p>[3] Official PyTorch Implementation of the Transformer Architecture. <em>GitHub repository</em>, PyTorch, <a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py" rel="external nofollow noopener" target="_blank">https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py</a></p>
<p>[4] Manning, Christopher, et al. ‚ÄúCS224n: Natural Language Processing with Deep Learning.‚Äù <em>Stanford University</em>, Stanford CS224N NLP course, <a href="http://web.stanford.edu/class/cs224n/" rel="external nofollow noopener" target="_blank">http://web.stanford.edu/class/cs224n/</a></p>
<p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=72d69e383e0d" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/transformers-how-do-they-transform-your-data-72d69e383e0d" rel="external nofollow noopener" target="_blank">Transformers: How Do They Transform Your Data?</a> was originally published in <a href="https://medium.com/data-science" rel="external nofollow noopener" target="_blank">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>
</body></html>
