<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body>
<h4>Learn how to build and train an LLM efficiently on your laptop</h4>
<p>ChatGPT is an incredibly powerful tool for generating human-like responses to natural language queries. It uses a state-of-the-art AI language model, GPT, to understand and respond to text inputs in a conversational manner. However, one drawback of using ChatGPT is that it is not an open-source tool. This can cause some concern if you worry about your data being leaked.</p>
<p>Fortunately, it is possible to build your own local “ChatGPT”! In this step-by-step guide, I will show you how to do so, by finetuning an open-source model and using public data.</p>
<figure><img alt="" src="https://cdn-images-1.medium.com/max/740/1*AU_vMHBkfGTIKJhEKW984A.png"><figcaption>Image by BotUp</figcaption></figure>
<h3>LLaMA: an LLM waiting for you!</h3>
<figure><img alt="" src="https://cdn-images-1.medium.com/max/788/1*d-4x5C8Oy6LdVkdipJ0bkg.png"><figcaption>Image by the author</figcaption></figure>
<p>LLama has been recently introduced by Meta. It is a large language model and it exists in different versions, depending on the number of parameters of the model: 7B, 13B, 33B, and 65B parameters. We will use the first version (<strong>7 billion parameters</strong>). It is more than enough to get very good results. With some additional tricks, you will see that you can easily fit this into your computer, or into Google Colab!</p>
<p><strong>What exactly is LLaMA?</strong></p>
<p>To make it simple, it has been trained by Meta to predict, when some text is given as input, what comes next. The idea is to finetune this model so that it can provide an <em>answer </em>to some <em>instructions and inputs.</em></p>
<p><strong>How can you have access to LLaMa?</strong></p>
<p>Well, the model was not designed to be open-source! But it turns out that it has leaked and you can now find it pretty much everywhere. For example, you can find it on huggingface, right <a href="https://huggingface.co/decapoda-research/llama-7b-hf" rel="external nofollow noopener" target="_blank">here</a>.</p>
<p>Great! Now let’s explain the tools we will use to optimize the finetuning of this large model.</p>
<h3>LoRA and quantization</h3>
<p>LoRA is the key to building your own ChatGPT-like model! LoRA stands for Low-Rank Approximation. Okay… you might wonder what that means.</p>
<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Ir4445O1wMmlCOYRONgsmA.png"><figcaption>Low-rank approximation with rank=k</figcaption></figure>
<p>During finetuning, weights are updated. The idea is to approximate the weights matrix M by eliminating some parameters. Well, we could remove random parameters, right? Absolutely, it is a good first step. But <strong>LoRA</strong> is doing more than that. Intuitively, the goal is to <strong>select the most important parameters</strong>! The low-rank approximation matrix is a simplified version of the original matrix that captures the most important information while discarding some of the less important details. If you are familiar with linear algebra, this is nothing but keeping the largest singular values of the matrix in its SVD. By doing this, we can drastically <strong>reduce the number of parameters and speed up the finetuning and inference process</strong>!</p>
<p>Finally, we can use <strong>int8 quantization</strong>. That means reducing the precision of a neural network’s parameters from 32-bit floating-point numbers (FP32) to 8-bit integers.</p>
<p>We can then combine both <strong>LoRA and quantization</strong> to further optimize memory usage and speed up finetuning and inference. These two steps are fundamental.</p>
<h3>Let’s dive into the code!</h3>
<p>We are ready to start coding our own ChatBot. To finetune the LLaMA model, we’ll use <strong>52K samples of data</strong>, which you can find right <a href="https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json" rel="external nofollow noopener" target="_blank">here</a>. It contains instructions, input (optional), and output (I recommend you check the file to see what it looks like). Of course, you can choose whatever file you want, depending on your needs!</p>
<p>First, let’s install everything we need, you can install this <strong>locally</strong> in a new environment, or in google colab. I personally did it in<strong> Google Colab</strong> but it’s exactly the same locally! To do so, instead of cloning the <em>alpaca-lora </em>repository like below, you can directly download the <em>alpaca_data.json</em> and drop it into your local repository.</p>
<p><em>Note: if you use Google Colab, don’t forget to </em><strong><em>use the provided GPU</em></strong><em>!</em></p>
<pre>git clone https://github.com/tloen/alpaca-lora.git<br>%cd alpaca-lora/<br><br>!pip install -q datasets loralib sentencepiece<br>!pip uninstall transformers<br>!pip install -q git+https://github.com/zphang/transformers@c3dc391<br>!pip install -q git+https://github.com/huggingface/peft.git<br>!pip install snscrape</pre>
<p>Let’s import the packages we need:</p>
<pre>import os<br>import torch<br>import torch.nn as nn<br>import bitsandbytes as bnb<br>from datasets import load_dataset<br>import transformers<br>from transformers import AutoTokenizer, AutoConfig, LLaMAForCausalLM, LLaMATokenizer<br>from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model, PeftModel</pre>
<p>We can now define our <strong>hyperparameters</strong> (you can of course adapt these)<strong> </strong>and <strong>load the LLaMA model</strong>. Then, we define the configuration for the LoRA transformation, using the PEFT library.</p>
<pre>MICRO_BATCH_SIZE = 8<br>BATCH_SIZE = 128<br>GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE<br>EPOCHS = 3 #<br>LEARNING_RATE = 2e-5 <br>CUTOFF_LEN = 256<br>LORA_R = 4  # the number of singular values to keep for LoRA<br>LORA_ALPHA = 16<br>LORA_DROPOUT = 0.05</pre>
<pre>model = LLaMAForCausalLM.from_pretrained(<br>    "decapoda-research/llama-7b-hf",<br>    load_in_8bit=True,<br>    device_map="auto",<br>)<br>tokenizer = LLaMATokenizer.from_pretrained(<br>    "decapoda-research/llama-7b-hf", add_eos_token=True<br>)<br><br>model = prepare_model_for_int8_training(model)<br><br>config = LoraConfig(<br>    r=LORA_R,<br>    lora_alpha=LORA_ALPHA,<br>    target_modules=["q_proj", "v_proj"],<br>    lora_dropout=LORA_DROPOUT,<br>    bias="none",<br>    task_type="CAUSAL_LM",<br>)<br>model = get_peft_model(model, config)<br>tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token<br>data = load_dataset("json", data_files="alpaca_data.json")</pre>
<p>Great! Now, let’s <strong>load the data</strong> and<strong> finetune our model</strong>. The <em>generate_prompt</em> function processes every sample of the dataset we have in order to return a single text as a string. You can adapt this function depending on the data you want to use to finetune the model.</p>
<pre><br>def generate_prompt(data_point):<br>    if data_point["input"]:<br>        return f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.<br>  ### Instruction:<br>  {data_point["instruction"]}<br>  ### Input:<br>  {data_point["input"]}<br>  ### Response:<br>  {data_point["output"]}"""<br><br>    else:<br>        return f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.<br>### Instruction:<br>{data_point["instruction"]}<br>### Response:<br>{data_point["output"]}"""<br><br><br>data = data.shuffle().map(<br>    lambda data_point: tokenizer(<br>        generate_prompt(data_point),<br>        truncation=True,<br>        max_length=CUTOFF_LEN,<br>        padding="max_length",<br>    )<br>)<br><br>trainer = transformers.Trainer(<br>    model=model,<br>    train_dataset=data["train"],<br>    args=transformers.TrainingArguments(<br>        per_device_train_batch_size=MICRO_BATCH_SIZE,<br>        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,<br>        warmup_steps=100,<br>        num_train_epochs=EPOCHS,<br>        learning_rate=LEARNING_RATE,<br>        fp16=True,<br>        logging_steps=1,<br>        output_dir="your_directory",<br>        save_total_limit=3,<br>    ),<br>    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),<br>)<br>model.config.use_cache = False<br>trainer.train(resume_from_checkpoint=False)<br><br>model.save_pretrained("your_directory")</pre>
<p>The training might take some time. With Google Colab Pro, it took approximately 3 hours for 2 epochs. It can seem very long but we don’t need too many epochs. We already have <strong>very good results with 2 or 3 epochs</strong> as you will see!</p>
<p><em>Note: you can push the model on your huggingface hub to load it whenever you need.</em></p>
<pre>from huggingface_hub import notebook_login<br><br>notebook_login()<br>model.push_to_hub("your_username/your_model_name", use_auth_token=True)<br>model.save_pretrained("your_username/your_model_name", use_auth_token=True)</pre>
<p>Then, you can directly load your model with:</p>
<pre>tokenizer = LLaMATokenizer.from_pretrained("decapoda-research/llama-7b-hf")<br>model = LLaMAForCausalLM.from_pretrained(<br>    "decapoda-research/llama-7b-hf",<br>    load_in_8bit=True,<br>    device_map="auto",<br>)<br>model = PeftModel.from_pretrained(model, "your_username/your_model_name")</pre>
<p>Let’s try our new model. The inference function takes as input an instruction and the optional input.</p>
<pre>def inference(instruction_prompt, input_prompt=''):<br><br>if input_prompt:<br>    template = f'''Below is an instruction that describes a task. Write a response that appropriately completes the request.<br><br>  ### Instruction:<br>  {instruction_prompt}<br><br>  ### Input:<br>  {input_prompt}<br><br>  ### Response:<br>  '''<br><br>  else:<br>    template = f'''Below is an instruction that describes a task. Write a response that appropriately completes the request.<br><br>  ### Instruction:<br>  {instruction_prompt}<br><br>  ### Response:<br>  '''<br><br>  inputs = tokenizer(<br>    template,<br>    return_tensors="pt",<br>  )<br>  input_ids = inputs["input_ids"].cuda()<br><br>  generation_config = GenerationConfig(<br>      temperature=0.6,<br>      top_p=0.95,<br>      repetition_penalty=1.15,<br>  )<br>  print("Generating...")<br>  generation_output = model.generate(<br>      input_ids=input_ids,<br>      generation_config=generation_config,<br>      return_dict_in_generate=True,<br>      output_scores=True,<br>      max_new_tokens=128,<br>  )<br><br>  result = ''.join([tokenizer.decode(s) for s in generation_output.sequences])<br>  result = result.partition('### Response:\n   ')[2]<br><br>  return result</pre>
<p>Good, we are done! Let’s try this out. Here are some examples of results I got, training with only 2 epochs:</p>
<pre>inference('Write a long story about a flying cat')</pre>
<pre>The sky was dark and ominous as the wind howled through the trees, sending <br>them swaying back and forth in its wake. A small black kitten sat on top of <br>one of the branches, watching the storm with wide eyes. It had been raining <br>for days now, and the little feline was getting tired of being cooped up <br>inside all day. As it watched the rain pour down from above, it suddenly felt <br>inspired to take flight! With a mighty leap, the kitten soared into the air, <br>letting the wind carry it higher and higher until it reached the clouds.</pre>
<pre>inference('Extract cities as a list from this sentence: ', 'As a student studying abroad, I flew from New York City to Paris, where I spent a week before taking a train to Barcelona for a few days, and then continued my journey to Rome.')</pre>
<pre>[New York City, Paris, Barcelona, Rome]</pre>
<p>As you can see, we have very satisfying results!</p>
<h3>Conclusion</h3>
<p>By using an open-source model available on huggingface, some data, and a few optimization techniques, we successfully built our own ChatGPT-like model! Isn’t this crazy?</p>
<p>As you can imagine, there are plenty of potential applications. Also, keep in mind that there are more and more open-source LLMs. You can certainly find an alternative to LLaMA to build your own chatbot!</p>
<p>As a next step, you could try to integrate this into a streamlit application so that you can directly interact and chat with your model.</p>
<p>Congrats! You reached the end of this article. I hope you enjoyed reading it. Feel free to leave a comment and reach out!</p>
<p><strong>References</strong></p>
<p>[1] Meta AI, <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/" rel="external nofollow noopener" target="_blank">Introducing LLaMA: A foundational, 65-billion-parameter large language model</a> February 24, 2023</p>
<p>[2] Edward J. Hu, Yelong Shen, Phillip Willis, and al., <a href="https://arxiv.org/abs/2106.09685" rel="external nofollow noopener" target="_blank">LoRA: Low-Rank Adaptation of Large Language Models</a></p>
<p>[3] code_your_own_ai, <a href="https://www.youtube.com/watch?v=YVU5wAA6Txo" rel="external nofollow noopener" target="_blank">PEFT LoRA Explained in Detail — Fine-Tune your LLM on your local GPU</a></p>
<p>[4] Martin Thissen, <a href="https://medium.com/@martin-thissen/llama-alpaca-fine-tuning-code-generation-ram-requirements-and-more-answers-to-your-questions-b7e20b28e9e1" rel="external nofollow noopener" target="_blank">Alpaca &amp; LLaMA: Answering All Your Questions</a></p>
<p>[5] Rohan Taori, Ishaan Gulrajani, Tianyi Zang, and al., <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" rel="external nofollow noopener" target="_blank">Alpaca: A Strong, Replicable Instruction-Following Model</a></p>
<p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=51541237062" width="1" height="1" alt=""></p>
</body></html>
